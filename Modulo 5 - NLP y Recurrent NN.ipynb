{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "\n",
    "Primero vamos a crear un modelo sencillo de Bag of Words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'las': 2, 'palabras': 2, 'se': 2, 'repiten': 2}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def BagofWords(text):\n",
    "    bag = {}\n",
    "    vocab = []\n",
    "    \n",
    "    for word in text.split(\" \"):\n",
    "        word=word.lower()\n",
    "        bag[word]=1\n",
    "        if word in vocab:\n",
    "            bag[word] += 1\n",
    "            \n",
    "        vocab.append(word)\n",
    "    \n",
    "    return bag\n",
    "\n",
    "text=\"Las palabras se repiten se repiten las palabras\"\n",
    "BagofWords(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "Vamos a usar el Movie Review Dataset de IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ArchivosDePrograma\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\datasets\\imdb.py:129: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "D:\\ArchivosDePrograma\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\datasets\\imdb.py:130: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 88584\n",
    "\n",
    "MAXLEN = 250\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
    "                                                    num_words=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218, 43)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0]), len(train_data[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos no todos los inputs tiene la misma longitud. Las redes neuronales no admiten inputs de distinta forma, por lo que hay que modificar los datos a una longitud específica.\n",
    " \n",
    " - Si la longitud es mayor a 250, se cortan palabras.\n",
    " - Si es inferior, se añaden 0s hasta que llegue a 250."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sequence.pad_sequences(train_data, MAXLEN)\n",
    "test_data = sequence.pad_sequences(test_data, MAXLEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
    "    tf.keras.layers.LSTM(32),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 32)          2834688   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,843,041\n",
      "Trainable params: 2,843,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se usa un embedding inicial para darle más sentido a los \"vectores\" de palabras que es cada input. El 32 es porque creamos vectores de dimensión 32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 19s 962us/sample - loss: 0.4297 - acc: 0.8065 - val_loss: 0.2944 - val_acc: 0.8800\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 16s 812us/sample - loss: 0.2401 - acc: 0.9082 - val_loss: 0.3071 - val_acc: 0.8894\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 17s 844us/sample - loss: 0.1856 - acc: 0.9298 - val_loss: 0.2768 - val_acc: 0.8912\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 17s 832us/sample - loss: 0.1525 - acc: 0.9470 - val_loss: 0.2738 - val_acc: 0.8890\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 17s 840us/sample - loss: 0.1302 - acc: 0.9545 - val_loss: 0.4140 - val_acc: 0.8690\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 18s 877us/sample - loss: 0.1130 - acc: 0.9611 - val_loss: 0.3524 - val_acc: 0.8876\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 17s 845us/sample - loss: 0.0981 - acc: 0.9673 - val_loss: 0.3242 - val_acc: 0.8834\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 17s 839us/sample - loss: 0.0874 - acc: 0.9726 - val_loss: 0.3387 - val_acc: 0.8900\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 17s 863us/sample - loss: 0.0748 - acc: 0.9760 - val_loss: 0.3515 - val_acc: 0.8766\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 19s 928us/sample - loss: 0.0691 - acc: 0.9790 - val_loss: 0.3774 - val_acc: 0.8860\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "\n",
    "history = model.fit(train_data, train_labels, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que aunque hagamos 10 epochs el valor de la validación no cambia demasiado. Esto significa que hay algo que debe ajustarse en el modelo. Por el momento lo dejaremos así."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 5s 180us/sample - loss: 0.4905 - acc: 0.8479\n",
      "[0.49047818992614745, 0.84788]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_data, test_labels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No está mal para una red tan simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hacer predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   1  17  13  40 477  35 477]\n"
     ]
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "\n",
    "def encode_text(text):\n",
    "    tokens = tf.keras.preprocessing.text.text_to_word_sequence(text)\n",
    "    # esto separa cada palabra de la frase\n",
    "    tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
    "    # esto asigna un numero a cada palabra según word_index\n",
    "    return sequence.pad_sequences([tokens], MAXLEN)[0]\n",
    "\n",
    "text = 'the movie was just amazing, so amazing'\n",
    "encoded = encode_text(text)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the movie was just amazing so amazing\n"
     ]
    }
   ],
   "source": [
    "# Ahora hacemos una al reves, le damos números y nos dice la frase\n",
    "\n",
    "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
    "\n",
    "def decode_integers(integers):\n",
    "    PAD = 0\n",
    "    text = \"\"\n",
    "    for num in integers:\n",
    "        if num != PAD:\n",
    "            text += reverse_word_index[num] + \" \"\n",
    "            \n",
    "    return text[:-1]\n",
    "\n",
    "print(decode_integers(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 89.44% positiva\n",
      "Review 63.33% negativa\n"
     ]
    }
   ],
   "source": [
    "# ahora a hacer una predicción\n",
    "\n",
    "def predict(text):\n",
    "    encoded_text = encode_text(text)\n",
    "    pred = np.zeros((1,250)) # el modelo espera esta estructura\n",
    "    pred[0] = encoded_text\n",
    "    result = model.predict(pred)\n",
    "    if result[0] >= 0.5:\n",
    "        print(\"Review {:.2f}% positiva\".format(result[0][0] * 100))\n",
    "    else:\n",
    "        print(\"Review {:.2f}% negativa\".format(100 - result[0][0] * 100))\n",
    "\n",
    "pos_review = \"That movie was so cool! I really loved it and would watch it again because it was amazingly great\"\n",
    "predict(pos_review)\n",
    "\n",
    "neg_review = \"That movie totally sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\n",
    "predict(neg_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escribir una obra con RNN\n",
    "\n",
    "Vamos a hacer una RNN que prediga la siguiente palabra, como el predictor de texto del movil. La vamos a entrenar con Romeo y Julieta de Shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file(\"shakespeare.txt\", \n",
    "                                      \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\jaime\\\\.keras\\\\datasets\\\\shakespeare.txt'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leer los contenidos del texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "print(\"Length of text: {} characters\".format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Codificar el texto\n",
    "\n",
    "Dado que aquí no tenemos un número asociado a cada palabra como teníamos antes, tenemos que hacerlo nosotros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))\n",
    "\n",
    "# creando un mapeado que asocie un número a cada palabra\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "def text_to_int(text):\n",
    "    return np.array([char2idx[c] for c in text])\n",
    "\n",
    "text_as_int = text_to_int(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos convertido cara caracter (no cada palabra) del texto en un número"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: First Citizen\n",
      "encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "print(\"text:\", text[:13])\n",
    "print(\"encoded:\",text_as_int[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y al contrario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen\n"
     ]
    }
   ],
   "source": [
    "def int_to_text(ints):\n",
    "    try:\n",
    "        ints = ints.numpy()\n",
    "    except:\n",
    "        pass\n",
    "    return ''.join(idx2char[ints])\n",
    "\n",
    "print(int_to_text(text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear ejemplos de entrenamiento\n",
    "\n",
    "El objetivo es darle al modelo una serie de caracteres y que nos de los siguientes. Por ello, tenemos que partir el texto grande en secuencias más pequeñas que darle al modelo como entrenamiento.\n",
    "\n",
    "Los ejemplos de entrenamiento serán el input que le pasemos con las letras desplazadas un espacio.\n",
    "\n",
    " - input: Hol | output: ola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text) // (seq_length + 1)\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividir entre input y output\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    output_text = chunk[1:]\n",
    "    return input_text, output_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "EXAMPLE\n",
      "\n",
      "INPUT\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "\n",
      "OUTPUT\n",
      "irst Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n",
      "\n",
      "\n",
      "EXAMPLE\n",
      "\n",
      "INPUT\n",
      "are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you \n",
      "\n",
      "OUTPUT\n",
      "re all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you k\n"
     ]
    }
   ],
   "source": [
    "for x,y in dataset.take(2):\n",
    "    print(\"\\n\\nEXAMPLE\\n\")\n",
    "    print(\"INPUT\")\n",
    "    print(int_to_text(x))\n",
    "    print(\"\\nOUTPUT\")\n",
    "    print(int_to_text(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tenemos que hacer batches de entrenamiento\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = len(vocab) # caracteres unicos\n",
    "EMBEDDING_DIM = 256\n",
    "RNN_UNITS = 1024\n",
    "\n",
    "# Tamaño de texto que barajar\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (64, None, 1024)          5246976   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                  batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.LSTM(rnn_units,\n",
    "                             return_sequences=True,\n",
    "                             stateful=True,\n",
    "                             recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear una función de pérdida\n",
    "\n",
    "Recordemos que el input al modelo (en entrenamiento) son objetos de longitud 100 en batches de 64. Cuando hagamos predicciones no siempre serán de esta longitud o forma. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in data.take(1):\n",
    "    example_batch_predictions = model(input_example_batch) \n",
    "    # prediccion en el primer batch de los datos de entrenamiento\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "# la prediccion es un array de 64, uno para cada entrada del batch\n",
    "print(len(example_batch_predictions))\n",
    "#print(example_batch_predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "pred = example_batch_predictions[0]\n",
    "print(len(pred))\n",
    "#print(pred)\n",
    "\n",
    "# es un array 2d de longitud 100, con cada array interior siendo las predicciones\n",
    "# para el siguiente elemento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "tf.Tensor(\n",
      "[-0.00175085 -0.00432556 -0.00706172 -0.001812   -0.00034917  0.00351697\n",
      " -0.0022645   0.00410583  0.00272123 -0.00171106  0.00333517  0.00290065\n",
      "  0.00039189 -0.00290298 -0.00153354  0.00026906 -0.00834936 -0.0004392\n",
      "  0.00074661  0.00657446  0.00539018 -0.00166748 -0.00534092 -0.00185909\n",
      "  0.00085378 -0.00276041 -0.00294825 -0.00536192 -0.00095577 -0.00364206\n",
      "  0.00096232 -0.00187163 -0.00188679 -0.00075335 -0.00551311 -0.0018262\n",
      " -0.00304138  0.00091418  0.00210359 -0.00223989 -0.00203788  0.00632498\n",
      "  0.00109678  0.00090947 -0.00492635 -0.00076982  0.00352446 -0.00183091\n",
      "  0.00241397  0.00202192  0.00291978 -0.0018658   0.00246556  0.00080082\n",
      " -0.00018268 -0.00219252  0.00136219 -0.00204161  0.00048571 -0.00018054\n",
      "  0.00083675 -0.00174689 -0.00155232  0.00104817  0.00372499], shape=(65,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# prediccion en el primer paso de tiempo\n",
    "time_pred = pred[0]\n",
    "print(len(time_pred))\n",
    "print(time_pred)\n",
    "\n",
    "# probabilidad de cada elemento en el primer paso de tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38 19 62  3 60 55  1 19 23 62 42 56 42 33  7 12 21 45 18 32 48  9 26 13\n",
      " 49  1 47 49 48 42  6 31 20 14 26 17 30 39 11 22 35 25 37 16 60 16  1 15\n",
      " 26 22 35 27 63 11 62 61 61 13 14  7  0 59  1 14 26  7 27 16 16  5 56 44\n",
      " 52 39 59 21 46 30 49 23  2  0 43 20 42  9 58  0  1  4 12 52 30  3 51 49\n",
      " 17 12 18 60]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"ZGx$vq GKxdrdU-?IgFTj3NAk ikjd,SHBNERa;JWMYDvD CNJWOy;xwwAB-\\nu BN-ODD'rfnauIhRkK!\\neHd3t\\n &?nR$mkE?Fv\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# se elige una muestra de las predicciones\n",
    "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
    "\n",
    "#los asociamos a cada letra del vocabulario\n",
    "sampled_indices = np.reshape(sampled_indices, (1,-1))[0]\n",
    "print(sampled_indices)\n",
    "predicted_chars = int_to_text(sampled_indices)\n",
    "\n",
    "predicted_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compilar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear checkpoints\n",
    "\n",
    "Esto permitirá tomar el modelo desde un checkpoint dado y seguir entrenándolo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 172 steps\n",
      "Epoch 1/100\n",
      "172/172 [==============================] - 21s 125ms/step - loss: 2.6867\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 18s 105ms/step - loss: 2.0051\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 18s 106ms/step - loss: 1.7370\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 1.5764\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 18s 105ms/step - loss: 1.4801\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 18s 105ms/step - loss: 1.4155\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 18s 105ms/step - loss: 1.3663\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 18s 105ms/step - loss: 1.3270\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 18s 107ms/step - loss: 1.2905\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 19s 112ms/step - loss: 1.2583\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 20s 115ms/step - loss: 1.2258\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 18s 107ms/step - loss: 1.1930\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 18s 106ms/step - loss: 1.1603\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 18s 106ms/step - loss: 1.1263\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 18s 107ms/step - loss: 1.0917\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 18s 107ms/step - loss: 1.0539\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 18s 107ms/step - loss: 1.0164\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 19s 108ms/step - loss: 0.9767\n",
      "Epoch 19/100\n",
      "172/172 [==============================] - 18s 106ms/step - loss: 0.9380\n",
      "Epoch 20/100\n",
      "172/172 [==============================] - 18s 105ms/step - loss: 0.8985\n",
      "Epoch 21/100\n",
      "172/172 [==============================] - 18s 105ms/step - loss: 0.8600\n",
      "Epoch 22/100\n",
      "172/172 [==============================] - 18s 106ms/step - loss: 0.8235\n",
      "Epoch 23/100\n",
      "172/172 [==============================] - 18s 107ms/step - loss: 0.7870\n",
      "Epoch 24/100\n",
      "172/172 [==============================] - 18s 106ms/step - loss: 0.7553\n",
      "Epoch 25/100\n",
      "172/172 [==============================] - 18s 107ms/step - loss: 0.7235\n",
      "Epoch 26/100\n",
      "172/172 [==============================] - 18s 105ms/step - loss: 0.6944\n",
      "Epoch 27/100\n",
      "172/172 [==============================] - 18s 105ms/step - loss: 0.6687\n",
      "Epoch 28/100\n",
      "172/172 [==============================] - 18s 105ms/step - loss: 0.6446\n",
      "Epoch 29/100\n",
      "172/172 [==============================] - 18s 105ms/step - loss: 0.6209\n",
      "Epoch 30/100\n",
      "172/172 [==============================] - 19s 111ms/step - loss: 0.6019\n",
      "Epoch 31/100\n",
      "172/172 [==============================] - 18s 106ms/step - loss: 0.5817\n",
      "Epoch 32/100\n",
      "172/172 [==============================] - 18s 107ms/step - loss: 0.5666\n",
      "Epoch 33/100\n",
      "172/172 [==============================] - 19s 110ms/step - loss: 0.5520\n",
      "Epoch 34/100\n",
      "172/172 [==============================] - 18s 106ms/step - loss: 0.5384\n",
      "Epoch 35/100\n",
      "172/172 [==============================] - 19s 108ms/step - loss: 0.5264\n",
      "Epoch 36/100\n",
      "172/172 [==============================] - 65s 375ms/step - loss: 0.5142\n",
      "Epoch 37/100\n",
      "172/172 [==============================] - 36s 208ms/step - loss: 0.5061\n",
      "Epoch 38/100\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 0.4955\n",
      "Epoch 39/100\n",
      "172/172 [==============================] - 18s 105ms/step - loss: 0.4852\n",
      "Epoch 40/100\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 0.4774\n",
      "Epoch 41/100\n",
      "172/172 [==============================] - 18s 105ms/step - loss: 0.4712\n",
      "Epoch 42/100\n",
      "172/172 [==============================] - 18s 105ms/step - loss: 0.46550s - loss: 0.46\n",
      "Epoch 43/100\n",
      "172/172 [==============================] - 18s 105ms/step - loss: 0.4607\n",
      "Epoch 44/100\n",
      "172/172 [==============================] - 18s 105ms/step - loss: 0.4529\n",
      "Epoch 45/100\n",
      "172/172 [==============================] - 18s 105ms/step - loss: 0.4480\n",
      "Epoch 46/100\n",
      "172/172 [==============================] - 18s 105ms/step - loss: 0.44360s - loss: 0.\n",
      "Epoch 47/100\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 0.4394\n",
      "Epoch 48/100\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 0.4357\n",
      "Epoch 49/100\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 0.4309\n",
      "Epoch 50/100\n",
      "172/172 [==============================] - 18s 107ms/step - loss: 0.4285\n",
      "Epoch 51/100\n",
      "172/172 [==============================] - 26s 150ms/step - loss: 0.4272\n",
      "Epoch 52/100\n",
      "172/172 [==============================] - 31s 183ms/step - loss: 0.4235\n",
      "Epoch 53/100\n",
      "172/172 [==============================] - 18s 103ms/step - loss: 0.4180\n",
      "Epoch 54/100\n",
      "172/172 [==============================] - 18s 103ms/step - loss: 0.4164\n",
      "Epoch 55/100\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 0.4138\n",
      "Epoch 56/100\n",
      "172/172 [==============================] - 18s 105ms/step - loss: 0.4117\n",
      "Epoch 57/100\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 0.4086\n",
      "Epoch 58/100\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 0.4070\n",
      "Epoch 59/100\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 0.4054\n",
      "Epoch 60/100\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 0.4053\n",
      "Epoch 61/100\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 0.4022\n",
      "Epoch 62/100\n",
      "172/172 [==============================] - 18s 103ms/step - loss: 0.4006\n",
      "Epoch 63/100\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 0.4013\n",
      "Epoch 64/100\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 0.4003\n",
      "Epoch 65/100\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 0.3957\n",
      "Epoch 66/100\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 0.3940\n",
      "Epoch 67/100\n",
      "172/172 [==============================] - 18s 106ms/step - loss: 0.3924\n",
      "Epoch 68/100\n",
      "172/172 [==============================] - 18s 106ms/step - loss: 0.3933\n",
      "Epoch 69/100\n",
      "172/172 [==============================] - 18s 106ms/step - loss: 0.3895\n",
      "Epoch 70/100\n",
      "172/172 [==============================] - 19s 110ms/step - loss: 0.3893\n",
      "Epoch 71/100\n",
      "172/172 [==============================] - 18s 105ms/step - loss: 0.3893\n",
      "Epoch 72/100\n",
      "172/172 [==============================] - 18s 105ms/step - loss: 0.3863\n",
      "Epoch 73/100\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 0.3858\n",
      "Epoch 74/100\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 0.3850\n",
      "Epoch 75/100\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 0.38531s - l\n",
      "Epoch 76/100\n",
      "172/172 [==============================] - 18s 105ms/step - loss: 0.3842\n",
      "Epoch 77/100\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 0.3826\n",
      "Epoch 78/100\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 0.38192s - l - ETA: 0s - los\n",
      "Epoch 79/100\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 0.3802\n",
      "Epoch 80/100\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 0.3803\n",
      "Epoch 81/100\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 0.3824\n",
      "Epoch 82/100\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 0.3810\n",
      "Epoch 83/100\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 0.3773\n",
      "Epoch 84/100\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 0.3785\n",
      "Epoch 85/100\n",
      "172/172 [==============================] - 18s 105ms/step - loss: 0.3775\n",
      "Epoch 86/100\n",
      "172/172 [==============================] - 18s 103ms/step - loss: 0.3750\n",
      "Epoch 87/100\n",
      "172/172 [==============================] - 18s 103ms/step - loss: 0.3766\n",
      "Epoch 88/100\n",
      "172/172 [==============================] - 18s 103ms/step - loss: 0.3771\n",
      "Epoch 89/100\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 0.3767\n",
      "Epoch 90/100\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 0.3743\n",
      "Epoch 91/100\n",
      "172/172 [==============================] - 18s 103ms/step - loss: 0.3734\n",
      "Epoch 92/100\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 0.3714\n",
      "Epoch 93/100\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 0.3705\n",
      "Epoch 94/100\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 0.36960s - loss: \n",
      "Epoch 95/100\n",
      "172/172 [==============================] - 18s 103ms/step - loss: 0.3689\n",
      "Epoch 96/100\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 0.3672\n",
      "Epoch 97/100\n",
      "172/172 [==============================] - 18s 103ms/step - loss: 0.3682\n",
      "Epoch 98/100\n",
      "172/172 [==============================] - 18s 103ms/step - loss: 0.3724\n",
      "Epoch 99/100\n",
      "172/172 [==============================] - 18s 105ms/step - loss: 0.3711\n",
      "Epoch 100/100\n",
      "172/172 [==============================] - 18s 105ms/step - loss: 0.3724\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(data, epochs=100, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargar el modelo\n",
    "\n",
    "Vamos a reconstruir el modelo desde un checkpoint usando un tamaño de batch 1. Esto nos permite darle al modelo un elemento de texto cada vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# el ultimo checkpoint del modelo es\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1,None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos cargar cualquier checkpoint con:\n",
    "\n",
    "    - model.load_weights(tf.train.load_checkpoint(\"./training_checkpoints/ckpt_\" + str(checkpoint_num)))\n",
    "    \n",
    "    - model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generar Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Paso de evaluacion\n",
    "\n",
    "  # Numero de caracteres a generar\n",
    "  num_generate = 800\n",
    "\n",
    "  # Convertir el string inicial en numeros\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # String vacio para guardar el resultado\n",
    "  text_generated = []\n",
    "\n",
    "  # Temperaturas bajas en textos predecibles\n",
    "  # Temperaturas altas en textos impredecibles\n",
    "  # Experimentar hasta encontrar el mejor\n",
    "  temperature = 0.8\n",
    "\n",
    "\n",
    "  # Tamaño de batch es 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # Quitar las dimensiones [[]]\n",
    "    \n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # Se usa una distribución categórica para predecir el output\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # Se pasa el caracter predicho como el siguiente input\n",
    "      # junto con todos los estados anteriores\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escribe la primera frase (en inglés): You are all resolved rather to die than to famish?\n",
      "\n",
      "\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "BUCKINGHAM:\n",
      "What says his titue on you of such thing world,\n",
      "And 'twill be withdraw together, let him be\n",
      "known the gates. This is the matter, many hearing of\n",
      "the souls of Richard makes a numbering creature king?\n",
      "Ere he did not, that act on 't: though it passabler,\n",
      "Nor had not off with maids to-morrow:\n",
      "Trong fair Tyrrelent did I drop Clarence,\n",
      "That love should purpose\n",
      "To counterfeit that, noble lo, a\n",
      "wicked vanity the helm and Richard like the town\n",
      "Thy brother's blood open it again: if you are too former\n",
      "And hang me: the rest her in the field.\n",
      "\n",
      "YORK:\n",
      "What bore may gentlemen, I have heard you say\n",
      "'That a Jack in them.\n",
      "\n",
      "First Senator:\n",
      "Be it so; if any were in love before I came.\n",
      "\n",
      "DUCHESS:\n",
      "Whose house, my lord, we would have had you heard\n",
      "The traitor speak bring\n",
      "Till\n"
     ]
    }
   ],
   "source": [
    "inp = input(\"Escribe la primera frase (en inglés): \")\n",
    "print(\"\\n\")\n",
    "print(generate_text(model, inp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mejorar resultados se puede añadir otra capa al modelo (LSTM o GRU), modificar valores de temperatura o añadir más epochs (aunque parece que con 100 se ha alcanzado una constante de loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
